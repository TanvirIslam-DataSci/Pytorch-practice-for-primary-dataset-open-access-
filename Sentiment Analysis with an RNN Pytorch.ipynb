{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with open('reviews.txt','r') as f:\n",
    "    reviews = f.read()\n",
    "with open('labels.txt','r') as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n",
      "story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turn\n",
      "\n",
      "positive\n",
      "negative\n",
      "po\n"
     ]
    }
   ],
   "source": [
    "print(reviews[:1000])\n",
    "print()\n",
    "print(labels[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  string import punctuation\n",
    "\n",
    "reviews = reviews.lower()\n",
    "all_text = ''.join([c for c in reviews if c not in punctuation])\n",
    "\n",
    "reviews_split = all_text.split('\\n')\n",
    "all_text = ' '.join(reviews_split)\n",
    "\n",
    "words = all_text.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bromwell',\n",
       " 'high',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cartoon',\n",
       " 'comedy',\n",
       " 'it',\n",
       " 'ran',\n",
       " 'at',\n",
       " 'the',\n",
       " 'same',\n",
       " 'time',\n",
       " 'as',\n",
       " 'some',\n",
       " 'other',\n",
       " 'programs',\n",
       " 'about',\n",
       " 'school',\n",
       " 'life',\n",
       " 'such',\n",
       " 'as',\n",
       " 'teachers',\n",
       " 'my',\n",
       " 'years',\n",
       " 'in',\n",
       " 'the',\n",
       " 'teaching',\n",
       " 'profession',\n",
       " 'lead',\n",
       " 'me']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word:ii for ii,word in enumerate(vocab,1)}\n",
    "\n",
    "reviews_ints = []\n",
    "for review in reviews_split:\n",
    "    reviews_ints.append([vocab_to_int[word] for word in review.split()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words:  53766\n",
      "\n",
      "Tokenized review: \n",
      " [[14010, 319, 6, 3, 1118, 202, 8, 2214, 32, 1, 168, 57, 15, 49, 78, 5681, 44, 420, 114, 140, 15, 4026, 61, 147, 9, 1, 4783, 5682, 481, 70, 5, 258, 12, 14010, 319, 13, 2263, 6, 73, 2507, 5, 593, 74, 6, 4026, 1, 17942, 5, 1889, 9261, 1, 5969, 1557, 36, 52, 65, 207, 145, 67, 1240, 4026, 19990, 1, 26843, 4, 1, 225, 881, 31, 3159, 70, 4, 1, 6119, 10, 703, 2, 67, 1557, 55, 10, 212, 1, 353, 9, 64, 3, 1390, 3633, 756, 5, 3542, 177, 1, 420, 10, 1187, 16357, 32, 319, 3, 347, 334, 3083, 10, 143, 129, 5, 7036, 30, 4, 130, 4026, 1390, 2452, 5, 14010, 319, 10, 525, 12, 110, 1403, 4, 61, 589, 102, 12, 14010, 319, 6, 234, 4164, 48, 3, 2453, 12, 8, 230, 21]]\n"
     ]
    }
   ],
   "source": [
    "print('Unique words: ', len((vocab_to_int)))\n",
    "print()\n",
    "print('Tokenized review: \\n', reviews_ints[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_split = labels.split('\\n')\n",
    "encoded_labels = np.array([1 if label == 'positive' else 0 for label in labels_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 0\n",
      "Maximum review length: 2514\n"
     ]
    }
   ],
   "source": [
    "review_lens = Counter([len(x) for x in reviews_ints])\n",
    "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews before removing outliers:  12110\n",
      "Number of reviews after removing outliers:  12110\n"
     ]
    }
   ],
   "source": [
    " print('Number of reviews before removing outliers: ', len(reviews_ints))\n",
    "\n",
    "non_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review) != 0]\n",
    "\n",
    "reviews_ints = [reviews_ints[ii] for ii in non_zero_idx]\n",
    "encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])\n",
    "\n",
    "print('Number of reviews after removing outliers: ', len(reviews_ints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(reviews_ints, seq_length):\n",
    "    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
    "    \n",
    "    for i,row in enumerate(reviews_ints):\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [16359    43 33699    15   740 13113  3677    47    75    35  1959    16\n",
      "    147    19   111     3  1279     5   336   146]]\n"
     ]
    }
   ],
   "source": [
    "seq_length = 200\n",
    "features = pad_features(reviews_ints, seq_length=seq_length)\n",
    "\n",
    "assert len(features) == len(reviews_ints), \"Your features should have as many rows as reviews.\"\n",
    "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
    "\n",
    "print(features[:3,:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t Feature Shapes:\n",
      "Train set: \t\t(9688, 200) \n",
      "Validation set: \t(1211, 200) \n",
      "Test set: \t\t(1211, 200)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.8\n",
    "split_idx = int(len(features)*split_frac)\n",
    "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
    "train_y , remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_x)*0.5)\n",
    "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "\n",
    "print(\"\\t\\t Feature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \"\\nValidation set: \\t{}\".format(val_x.shape), \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data =  TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=50)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=50)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([50, 200])\n",
      "Sample input \n",
      " tensor([[ 624,  999,  562,  ...,   24,   15, 1302],\n",
      "        [   0,    0,    0,  ...,   57,    5,  105],\n",
      "        [  10,  143,   24,  ...,   41,   13,   53],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ...,    1,  658,  114],\n",
      "        [   0,    0,    0,  ...,   21,  105,   11],\n",
      "        [  39,   10,  301,  ...,  716,  281,   22]], dtype=torch.int32)\n",
      "\n",
      "Sample label size:  torch.Size([50])\n",
      "Sample label: \n",
      " tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,\n",
      "        0, 1], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(valid_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size())\n",
    "print('Sample input \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size())\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        super(SentimentRNN,self).__init__()\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.long()\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        lstm_out = lstm_out[:, -1, : ]\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        return sig_out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embedding): Embedding(53767, 400)\n",
      "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab_to_int)+1\n",
    "output_size = 1\n",
    "embedding_dim = 400\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "\n",
    "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ëpoch: 1/4... Step: 100.. Loss: 0.608746... Val Loss: 0.651273\n",
      "Ëpoch: 2/4... Step: 200.. Loss: 0.686467... Val Loss: 0.693858\n",
      "Ëpoch: 2/4... Step: 300.. Loss: 0.635827... Val Loss: 0.651590\n",
      "Ëpoch: 3/4... Step: 400.. Loss: 0.522323... Val Loss: 0.624199\n",
      "Ëpoch: 3/4... Step: 500.. Loss: 0.797201... Val Loss: 0.798805\n",
      "Ëpoch: 4/4... Step: 600.. Loss: 0.217707... Val Loss: 0.682495\n",
      "Ëpoch: 4/4... Step: 700.. Loss: 0.297671... Val Loss: 0.655398\n"
     ]
    }
   ],
   "source": [
    "epochs = 4\n",
    "batch_size = 50\n",
    "counter = 0\n",
    "print_every = 100\n",
    "\n",
    "clip = 5\n",
    "\n",
    "net.train()\n",
    "\n",
    "for e in range(epochs):\n",
    "    \n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        if(len(inputs)<50):\n",
    "                    break\n",
    "        counter += 1\n",
    "        \n",
    "        h = tuple([each.data for each in h])\n",
    "        \n",
    "        net.zero_grad()\n",
    "        output, h = net(inputs, h)\n",
    "        #print(len(h), len(output))\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        \n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if counter % print_every == 0:\n",
    "            val_h = net.init_hidden(50)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inp, labels in valid_loader:\n",
    "                if(len(inp)<50):\n",
    "                    break\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                \n",
    "                output, val_h = net(inp, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "            net.train()\n",
    "            print(\"Ëpoch: {}/{}...\".format(e+1, epochs),\n",
    "                 \"Step: {}..\".format(counter),\n",
    "                 \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                 \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.756\n",
      "Test accuracy: 0.724\n"
     ]
    }
   ],
   "source": [
    "test_losses = []\n",
    "num_correct = 0\n",
    "\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "    if(len(inputs)<50):\n",
    "        break\n",
    "    h = tuple([each.data for each in h])\n",
    "    output, h = net(inputs, h)\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    pred = torch.round(output.squeeze())\n",
    "    \n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_review_neg = 'The Best movie I have seen; acting was terrible. This movie had good acting and the dialogue was fantastic.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 116, 18, 10, 28, 109, 115, 14, 372, 11, 18, 68, 51, 115, 2, 1, 403, 14, 710]]\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "def tokenize_review(test_review):\n",
    "    test_review = test_review.lower()\n",
    "    test_text = ''.join([c for c in test_review if c not in punctuation])\n",
    "    \n",
    "    test_words = test_text.split()\n",
    "    \n",
    "    test_ints = []\n",
    "    test_ints.append([vocab_to_int.get(word, 0 ) for word in test_words])\n",
    "    \n",
    "    return test_ints\n",
    "\n",
    "test_ints = tokenize_review(test_review_neg)\n",
    "print(test_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   1 116  18  10  28 109 115  14 372  11  18  68  51 115   2   1 403\n",
      "   14 710]]\n"
     ]
    }
   ],
   "source": [
    "seq_length = 200\n",
    "features = pad_features(test_ints, seq_length)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net , test_review, sequence_length=200):\n",
    "    net.eval()\n",
    "    test_ints = tokenize_review(test_review)\n",
    "    \n",
    "    seq_length = sequence_length\n",
    "    \n",
    "    features = pad_features(test_ints, seq_length)\n",
    "    feature_tensor = torch.from_numpy(features)\n",
    "    batch_size = feature_tensor.size(0)\n",
    "    \n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    output, h  = net(feature_tensor, h)\n",
    "    \n",
    "    pred = torch.round(output.squeeze())\n",
    "    print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n",
    "    \n",
    "    if(pred.item()==1):\n",
    "        print(\"Positive review\")\n",
    "    else:\n",
    "        print(\"Negative review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction value, pre-rounding: 0.504346\n",
      "Positive review\n"
     ]
    }
   ],
   "source": [
    "predict(net, test_review_neg, sequence_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
