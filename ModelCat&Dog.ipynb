{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR= r\"C:\\Users\\tanvi\\dataset\\training_set\"\n",
    "CATEGORIES = [\"dogs\",\"cats\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=[]\n",
    "\n",
    "def prepare_train_data():\n",
    "    for category in CATEGORIES:\n",
    "        path = os.path.join(DATADIR, category)\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_GRAYSCALE)\n",
    "                new_array = cv2.resize(img_array,(50,50))\n",
    "                training_data.append([new_array, class_num])\n",
    "            except Exception as e:\n",
    "                pass\n",
    " \n",
    "prepare_train_data()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "random.shuffle(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for sample in training_data[:10]:\n",
    "    print(sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= []\n",
    "Y= []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature, label in training_data:\n",
    "    X.append(feature)\n",
    "    Y.append(label)\n",
    "    \n",
    "X = np.array(X).reshape(-1, 50, 50, 1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (2500, 8000)\n"
     ]
    }
   ],
   "source": [
    "train_x_flatten = X.reshape(X.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "\n",
    "\n",
    "print (\"train_x's shape: \" + str(train_x.shape))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for 2 layer model\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    np.random.seed(1)\n",
    "\n",
    "    W1 = np.random.randn(n_h,n_x) *0.01\n",
    "    b1= np.zeros(shape=(n_h,1))\n",
    "    W2= np.random.randn(n_y,n_h)* 0.01\n",
    "    b2 = np.zeros(shape=(n_y,1))\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A,W,b):\n",
    "\n",
    "    Z = W @ A + b\n",
    "    cache = (A,W,b)\n",
    "\n",
    "    return Z,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "\n",
    "    if activation==\"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation== \"relu\":\n",
    "        Z,linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost calculation\n",
    "def compute_cost(AL, Y):\n",
    "\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    cost = -1/m * np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL))\n",
    "\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backpropagation\n",
    "def linear_backward(dZ, cache):\n",
    "\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1/m* dZ @ A_prev.T\n",
    "    db = 1/m* np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = W.T @ dZ\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "\n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    elif activation ==\"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    return dA_prev, dW, db    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "\n",
    "    L= len(parameters) //2\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] -learning_rate * grads[\"dW\"+str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] -learning_rate * grads[\"db\"+str(l+1)]\n",
    "    return parameters   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x = 2500     # num_px * num_px * 3\n",
    "n_h = 7\n",
    "n_y = 1\n",
    "layers_dims = (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: two_layer_model\n",
    "\n",
    "def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a two-layer neural network: LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- If set to True, this will print the cost every 100 iterations \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary containing W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []                              # to keep track of the cost\n",
    "    m = X.shape[1]                           # number of examples\n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "    \n",
    "    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    print(len(parameters))\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1, W2, b2\". Output: \"A1, cache1, A2, cache2\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A1, cache1 = linear_activation_forward(X, W1, b1,activation='relu')\n",
    "        A2, cache2 = linear_activation_forward(A1, W2, b2,activation='sigmoid')\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute cost\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = compute_cost(A2,Y)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Initializing backward propagation\n",
    "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "        \n",
    "        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, activation='sigmoid')\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, activation='relu')\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (approx. 1 line of code)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Retrieve W1, b1, W2, b2 from parameters\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "       \n",
    "    # plot the cost\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 8000)\n",
      "(1, 8000)\n"
     ]
    }
   ],
   "source": [
    "Y = np.array(Y).reshape(1,8000)\n",
    "print(train_x.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Cost after iteration 0: 0.6931418428499108\n",
      "Cost after iteration 100: 0.6931149026458432\n",
      "Cost after iteration 200: 0.6930851378745699\n",
      "Cost after iteration 300: 0.6930497731480438\n",
      "Cost after iteration 400: 0.693004918195444\n",
      "Cost after iteration 500: 0.6929457299954825\n",
      "Cost after iteration 600: 0.6928666447177583\n",
      "Cost after iteration 700: 0.6927622128818463\n",
      "Cost after iteration 800: 0.6926202328166557\n",
      "Cost after iteration 900: 0.6924326686274115\n",
      "Cost after iteration 1000: 0.692185762106272\n",
      "Cost after iteration 1100: 0.6918625566915164\n",
      "Cost after iteration 1200: 0.6914464597472124\n",
      "Cost after iteration 1300: 0.6909207723013965\n",
      "Cost after iteration 1400: 0.6902692403531284\n",
      "Cost after iteration 1500: 0.6894877261551756\n",
      "Cost after iteration 1600: 0.6885771601300861\n",
      "Cost after iteration 1700: 0.6875522363337822\n",
      "Cost after iteration 1800: 0.6864377676469962\n",
      "Cost after iteration 1900: 0.6852578656269679\n",
      "Cost after iteration 2000: 0.6840383338140877\n",
      "Cost after iteration 2100: 0.6828132969058197\n",
      "Cost after iteration 2200: 0.6816051643595613\n",
      "Cost after iteration 2300: 0.6804280638474318\n",
      "Cost after iteration 2400: 0.6792861543346912\n",
      "Cost after iteration 2500: 0.6781749162938993\n",
      "Cost after iteration 2600: 0.6770748675890947\n",
      "Cost after iteration 2700: 0.6759999908476065\n",
      "Cost after iteration 2800: 0.6749457660640897\n",
      "Cost after iteration 2900: 0.6739201197165029\n",
      "Cost after iteration 3000: 0.672926002325198\n",
      "Cost after iteration 3100: 0.6719644777166484\n",
      "Cost after iteration 3200: 0.6710214139425454\n",
      "Cost after iteration 3300: 0.6700834290162351\n",
      "Cost after iteration 3400: 0.6691548683477481\n",
      "Cost after iteration 3500: 0.668211004392776\n",
      "Cost after iteration 3600: 0.6672028250355929\n",
      "Cost after iteration 3700: 0.6662031045904998\n",
      "Cost after iteration 3800: 0.665173302285054\n",
      "Cost after iteration 3900: 0.6641079405909556\n",
      "Cost after iteration 4000: 0.6630324587514961\n",
      "Cost after iteration 4100: 0.6619181136598046\n",
      "Cost after iteration 4200: 0.6607702496008455\n",
      "Cost after iteration 4300: 0.6596258415038617\n",
      "Cost after iteration 4400: 0.6585050666286762\n",
      "Cost after iteration 4500: 0.6573983824836787\n",
      "Cost after iteration 4600: 0.6562752901615856\n",
      "Cost after iteration 4700: 0.6551471152912617\n",
      "Cost after iteration 4800: 0.6540044009960009\n",
      "Cost after iteration 4900: 0.6528930807773914\n",
      "Cost after iteration 5000: 0.6518123613070043\n",
      "Cost after iteration 5100: 0.6507407827925171\n",
      "Cost after iteration 5200: 0.6496822819700405\n",
      "Cost after iteration 5300: 0.6486426080602482\n",
      "Cost after iteration 5400: 0.6476291338426124\n",
      "Cost after iteration 5500: 0.646636810887951\n",
      "Cost after iteration 5600: 0.6456586326234147\n",
      "Cost after iteration 5700: 0.6446908835296029\n",
      "Cost after iteration 5800: 0.6437489383512195\n",
      "Cost after iteration 5900: 0.6428182938222075\n",
      "Cost after iteration 6000: 0.6419053096229991\n",
      "Cost after iteration 6100: 0.6410071693740458\n",
      "Cost after iteration 6200: 0.6401265209041626\n",
      "Cost after iteration 6300: 0.639241666789211\n",
      "Cost after iteration 6400: 0.6383396276513447\n",
      "Cost after iteration 6500: 0.6374207488370469\n",
      "Cost after iteration 6600: 0.6365134424141561\n",
      "Cost after iteration 6700: 0.6356129566994291\n",
      "Cost after iteration 6800: 0.6347188570692694\n",
      "Cost after iteration 6900: 0.6338282455841874\n",
      "Cost after iteration 7000: 0.6329387223355202\n",
      "Cost after iteration 7100: 0.6320461482047924\n",
      "Cost after iteration 7200: 0.6311538997051364\n",
      "Cost after iteration 7300: 0.6302598693514001\n",
      "Cost after iteration 7400: 0.6293546173859698\n",
      "Cost after iteration 7500: 0.6284497426301793\n",
      "Cost after iteration 7600: 0.6275202807358015\n",
      "Cost after iteration 7700: 0.6265724047961269\n",
      "Cost after iteration 7800: 0.6256167412264511\n",
      "Cost after iteration 7900: 0.6246488807909184\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = two_layer_model(train_x, Y, layers_dims = (n_x, n_h, n_y), num_iterations = 8000, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6566250000000002\n"
     ]
    }
   ],
   "source": [
    "predictions_train = predict(train_x, Y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
